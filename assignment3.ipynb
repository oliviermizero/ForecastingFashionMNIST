{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install idx2numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LTg3SFUfZ3W",
        "outputId": "f5585ee9-cdd8-4d62-e3e3-c09884d3371d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: idx2numpy in /usr/local/lib/python3.10/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from idx2numpy) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from idx2numpy) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vaDhdnYUPQ-x"
      },
      "outputs": [],
      "source": [
        "# For reading data\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# For visualizing\n",
        "import plotly.express as px\n",
        "\n",
        "# For model building\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import idx2numpy\n",
        "import requests\n",
        "import gzip\n",
        "import numpy as np\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNIST(Dataset):\n",
        "    def __init__(self, images_url, labels_url):\n",
        "        # Download and read in our raw data from the IDX files\n",
        "        self.images = self.download_and_load_idx(images_url)\n",
        "        self.labels = self.download_and_load_idx(labels_url)\n",
        "\n",
        "    def download_and_load_idx(self, url):\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with gzip.GzipFile(fileobj=BytesIO(response.content)) as f:\n",
        "            return idx2numpy.convert_from_file(f)\n",
        "\n",
        "    # return the length of the complete data set\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    # retrieve a single record based on index position `idx`\n",
        "    def __getitem__(self, idx):\n",
        "        # extract the image and reshape it\n",
        "        image = self.images[idx].reshape(1, 28, 28)\n",
        "        # Specify dtype to align with default dtype used by weight matrices\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        # extract the label\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # return the image and its corresponding label\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "rkLOd4QAPfEl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, device):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "def train_net(model, train_dataloader, test_dataloader, epochs=5, learning_rate=1e-3, batch_size=64):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    lr = learning_rate\n",
        "    bs = batch_size\n",
        "    ep = epochs\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    for t in range(ep):\n",
        "        try:\n",
        "            print(f\"Epoch {model.EPOCH+t+1}\\n-------------------------------\")\n",
        "        except AttributeError:\n",
        "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
        "        test_loop(test_dataloader, model, loss_fn, device)\n",
        "    print(\"Done!\")\n",
        "\n",
        "    try:\n",
        "        model.EPOCH += ep\n",
        "    except AttributeError:\n",
        "        model.EPOCH = ep\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "etUrHOEAuTpm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URLs for the datasets\n",
        "train_images_url = \"https://github.com/oliviermizero/ForecastingFashionMNIST/raw/main/data/train-images-idx3-ubyte.gz\"\n",
        "train_labels_url = \"https://github.com/oliviermizero/ForecastingFashionMNIST/raw/main/data/train-labels-idx1-ubyte.gz\"\n",
        "test_images_url = \"https://github.com/oliviermizero/ForecastingFashionMNIST/raw/main/data/t10k-images-idx3-ubyte.gz\"\n",
        "test_labels_url = \"https://github.com/oliviermizero/ForecastingFashionMNIST/raw/main/data/t10k-labels-idx1-ubyte.gz\"\n",
        "\n",
        "# Load our data into memory\n",
        "train_data = FashionMNIST(train_images_url, train_labels_url)\n",
        "test_data = FashionMNIST(test_images_url, test_labels_url)\n",
        "\n",
        "# Create data feed pipelines for modeling\n",
        "train_dataloader = DataLoader(train_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)"
      ],
      "metadata": {
        "id": "RH9xFXk6Rknn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
        "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
        "        super(Residual, self).__init__()\n",
        "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
        "                                   stride=strides)\n",
        "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
        "                                       stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)"
      ],
      "metadata": {
        "id": "7QawhQ9xcyRJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.net = nn.Sequential(self.b1())\n",
        "        for i, b in enumerate(arch):\n",
        "            self.net.add_module(f'b{i+2}',\n",
        "                self.block(*b, first_block=(i==0)))\n",
        "        self.net.add_module('last', nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "            nn.LazyLinear(num_classes)))\n",
        "\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "    def block(self, num_residuals, num_channels, first_block=False):\n",
        "        blk = []\n",
        "        for i in range(num_residuals):\n",
        "            if i == 0 and not first_block:\n",
        "                blk.append(Residual(num_channels,\n",
        "                 use_1x1conv=True, strides=2))\n",
        "            else:\n",
        "                blk.append(Residual(num_channels))\n",
        "        return nn.Sequential(*blk)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "dqKUFkZmbXrx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18(ResNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super(ResNet18, self).__init__(((2, 64), (2, 128),\n",
        "         (2, 256), (2, 512)),\n",
        "                       lr, num_classes)\n",
        "\n",
        "model = ResNet18()#.to('cuda')"
      ],
      "metadata": {
        "id": "1CHa9CkXfHJO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_net(model, train_dataloader,\n",
        "        test_dataloader, epochs = 1, learning_rate = 1e-3,\n",
        "        batch_size=64\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XltDjsRWfKQG",
        "outputId": "05a92f4f-cfbf-495a-f94e-62d245ba3651"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.551995  [   64/60000]\n",
            "loss: 1.703794  [  704/60000]\n",
            "loss: 1.206934  [ 1344/60000]\n",
            "loss: 1.068375  [ 1984/60000]\n",
            "loss: 0.862926  [ 2624/60000]\n",
            "loss: 0.807481  [ 3264/60000]\n",
            "loss: 0.757776  [ 3904/60000]\n",
            "loss: 0.841575  [ 4544/60000]\n",
            "loss: 0.845504  [ 5184/60000]\n",
            "loss: 0.671600  [ 5824/60000]\n",
            "loss: 0.768402  [ 6464/60000]\n",
            "loss: 0.683704  [ 7104/60000]\n",
            "loss: 0.584607  [ 7744/60000]\n",
            "loss: 0.685592  [ 8384/60000]\n",
            "loss: 0.590827  [ 9024/60000]\n",
            "loss: 0.775680  [ 9664/60000]\n",
            "loss: 0.521001  [10304/60000]\n",
            "loss: 0.621127  [10944/60000]\n",
            "loss: 0.801205  [11584/60000]\n",
            "loss: 0.481559  [12224/60000]\n",
            "loss: 0.398155  [12864/60000]\n",
            "loss: 0.640368  [13504/60000]\n",
            "loss: 0.363139  [14144/60000]\n",
            "loss: 0.443561  [14784/60000]\n",
            "loss: 0.488652  [15424/60000]\n",
            "loss: 0.558743  [16064/60000]\n",
            "loss: 0.511943  [16704/60000]\n",
            "loss: 0.498881  [17344/60000]\n",
            "loss: 0.601498  [17984/60000]\n",
            "loss: 0.575994  [18624/60000]\n",
            "loss: 0.600253  [19264/60000]\n",
            "loss: 0.684256  [19904/60000]\n",
            "loss: 0.366160  [20544/60000]\n",
            "loss: 0.373274  [21184/60000]\n",
            "loss: 0.540509  [21824/60000]\n",
            "loss: 0.271220  [22464/60000]\n",
            "loss: 0.529699  [23104/60000]\n",
            "loss: 0.437717  [23744/60000]\n",
            "loss: 0.519840  [24384/60000]\n",
            "loss: 0.450613  [25024/60000]\n",
            "loss: 0.527809  [25664/60000]\n",
            "loss: 0.613859  [26304/60000]\n",
            "loss: 0.451540  [26944/60000]\n",
            "loss: 0.488098  [27584/60000]\n",
            "loss: 0.457454  [28224/60000]\n",
            "loss: 0.466959  [28864/60000]\n",
            "loss: 0.635577  [29504/60000]\n",
            "loss: 0.610151  [30144/60000]\n",
            "loss: 0.482903  [30784/60000]\n",
            "loss: 0.259034  [31424/60000]\n",
            "loss: 0.492605  [32064/60000]\n",
            "loss: 0.363932  [32704/60000]\n",
            "loss: 0.329615  [33344/60000]\n",
            "loss: 0.532187  [33984/60000]\n",
            "loss: 0.483711  [34624/60000]\n",
            "loss: 0.330007  [35264/60000]\n",
            "loss: 0.380509  [35904/60000]\n",
            "loss: 0.495031  [36544/60000]\n",
            "loss: 0.289585  [37184/60000]\n",
            "loss: 0.339715  [37824/60000]\n",
            "loss: 0.415900  [38464/60000]\n",
            "loss: 0.313228  [39104/60000]\n",
            "loss: 0.323020  [39744/60000]\n",
            "loss: 0.336198  [40384/60000]\n",
            "loss: 0.455625  [41024/60000]\n",
            "loss: 0.369018  [41664/60000]\n",
            "loss: 0.491123  [42304/60000]\n",
            "loss: 0.438375  [42944/60000]\n",
            "loss: 0.305014  [43584/60000]\n",
            "loss: 0.298867  [44224/60000]\n",
            "loss: 0.482439  [44864/60000]\n",
            "loss: 0.397611  [45504/60000]\n",
            "loss: 0.608918  [46144/60000]\n",
            "loss: 0.505581  [46784/60000]\n",
            "loss: 0.470655  [47424/60000]\n",
            "loss: 0.410251  [48064/60000]\n",
            "loss: 0.471209  [48704/60000]\n",
            "loss: 0.375280  [49344/60000]\n",
            "loss: 0.498888  [49984/60000]\n",
            "loss: 0.388143  [50624/60000]\n",
            "loss: 0.463199  [51264/60000]\n",
            "loss: 0.315154  [51904/60000]\n",
            "loss: 0.463688  [52544/60000]\n",
            "loss: 0.592832  [53184/60000]\n",
            "loss: 0.292708  [53824/60000]\n",
            "loss: 0.419356  [54464/60000]\n",
            "loss: 0.352193  [55104/60000]\n",
            "loss: 0.463548  [55744/60000]\n",
            "loss: 0.329168  [56384/60000]\n",
            "loss: 0.370806  [57024/60000]\n",
            "loss: 0.517132  [57664/60000]\n",
            "loss: 0.213238  [58304/60000]\n",
            "loss: 0.406130  [58944/60000]\n",
            "loss: 0.625952  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.398102 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save our model for later, so we can train more or make predictions\n",
        "\n",
        "EPOCH = model.epochs\n",
        "# We use the .pt file extension by convention for saving\n",
        "#    pytorch models\n",
        "PATH = \"/content/sample_data/model.pt\"\n",
        "\n",
        "# The save function creates a binary storing all our data for us\n",
        "torch.save({\n",
        "            'epoch': EPOCH,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "CVB3JbZvv-S3",
        "outputId": "2da3bb2b-f5e7-4f1c-c4a6-2089ce1402df"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ResNet18' object has no attribute 'epochs'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f7f86466b36f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save our model for later, so we can train more or make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mEPOCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# We use the .pt file extension by convention for saving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#    pytorch models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ResNet18' object has no attribute 'epochs'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH"
      ],
      "metadata": {
        "id": "lINuWUXLR0Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify our path\n",
        "PATH = \"/content/sample_data/model.pt\"\n",
        "\n",
        "# Create a new \"blank\" model to load our information into\n",
        "blank_model = ResNet18()\n",
        "\n",
        "# Recreate our optimizer\n",
        "optimizer = torch.optim.SGD(blank_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Load back all of our data from the file\n",
        "checkpoint = torch.load(PATH)\n",
        "blank_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "EPOCH = checkpoint['epoch']"
      ],
      "metadata": {
        "id": "2M8ZURheQ4GH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}